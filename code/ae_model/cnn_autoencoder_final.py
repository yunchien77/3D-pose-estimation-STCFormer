# -*- coding: utf-8 -*-
"""cnn-autoencoder-final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CM3NLfhzGoQPxCffuRGKP8EHdJibdKSe
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from scipy.interpolate import interp1d
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, UpSampling1D, Flatten, Dropout, BatchNormalization, LeakyReLU, Add, Concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
import tensorflow as tf
import keras
from keras.utils import plot_model
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from sklearn.metrics import mean_squared_error, r2_score
from scipy.stats import pearsonr

# 設定隨機種子
np.random.seed(42)
tf.random.set_seed(42)

# 插值函數
def interpolate_ignore_nans(data, target_length, kind='linear'):
    def interpolate_row(row):
        mask = ~np.isnan(row)
        x_old = np.arange(len(row))
        if np.sum(mask) > 1:
            x_valid = x_old[mask]
            y_valid = row[mask]
            f = interp1d(x_valid, y_valid, kind=kind, fill_value="extrapolate")
            x_new = np.linspace(0, len(x_valid) - 1, target_length)
            return f(x_new)
        else:
            return np.full(target_length, np.nan)
    return np.apply_along_axis(interpolate_row, axis=1, arr=data)

# 自定義損失函數：結合MSE和峰值檢測損失
def custom_peak_loss(y_true, y_pred):
    # MSE損失
    mse_loss = tf.reduce_mean(tf.square(y_true - y_pred))

    # 計算一階導數
    dy_true = y_true[:, 1:] - y_true[:, :-1]
    dy_pred = y_pred[:, 1:] - y_pred[:, :-1]

    # 導數的MSE損失（保持波形特徵）
    gradient_loss = tf.reduce_mean(tf.square(dy_true - dy_pred))

    # 計算二階導數（捕捉曲率變化）
    d2y_true = dy_true[:, 1:] - dy_true[:, :-1]
    d2y_pred = dy_pred[:, 1:] - dy_pred[:, :-1]
    curvature_loss = tf.reduce_mean(tf.square(d2y_true - d2y_pred))

    # 組合損失
    total_loss = mse_loss + 0.3 * gradient_loss + 0.1 * curvature_loss
    return total_loss

# 改進的數據預處理函數
def preprocess_data(data, window_size=5):
    # 中值濾波去除異常值
    def median_filter(x, k=window_size):
        return pd.Series(x).rolling(k, center=True).median().fillna(method='bfill').fillna(method='ffill').values

    # 對每個時間序列應用中值濾波
    filtered_data = np.apply_along_axis(median_filter, 1, data)
    return filtered_data

# 簡化的數據增強函數 - 只添加噪聲
def augment_data(X, y, num_augmentations=1):
    X_aug, y_aug = [X], [y]

    for _ in range(num_augmentations):
        # 添加高斯噪聲，使用較小的噪聲級別
        noise_level = np.random.uniform(0.005, 0.015)
        X_noised = X + np.random.normal(0, noise_level, X.shape)

        X_aug.append(X_noised)
        y_aug.append(y)

    return np.concatenate(X_aug), np.concatenate(y_aug)

# 改進的殘差塊
def residual_block(x, filters, kernel_size=3):
    shortcut = x

    x = Conv1D(filters, kernel_size, padding='same', kernel_regularizer=l2(1e-4))(x)
    x = BatchNormalization()(x)
    x = LeakyReLU(0.2)(x)

    x = Conv1D(filters, kernel_size, padding='same', kernel_regularizer=l2(1e-4))(x)
    x = BatchNormalization()(x)

    if shortcut.shape[-1] != filters:
        shortcut = Conv1D(filters, 1, padding='same')(shortcut)

    x = Add()([x, shortcut])
    x = LeakyReLU(0.2)(x)
    return x

# 改進的自編碼器模型
def create_improved_autoencoder(input_shape):
    input_layer = Input(shape=(input_shape, 1))

    # Encoder
    x = Conv1D(32, 3, padding='same')(input_layer)
    x = BatchNormalization()(x)
    x = LeakyReLU(0.2)(x)
    x = MaxPooling1D(2, padding='same')(x)  # 500

    x = Conv1D(64, 3, padding='same')(x)
    x = BatchNormalization()(x)
    x = LeakyReLU(0.2)(x)
    x = MaxPooling1D(2, padding='same')(x)  # 250

    x = Conv1D(128, 3, padding='same')(x)
    x = BatchNormalization()(x)
    x = LeakyReLU(0.2)(x)
    encoded = MaxPooling1D(2, padding='same')(x)  # 125

    # Decoder
    x = Conv1D(128, 3, padding='same')(encoded)
    x = BatchNormalization()(x)
    x = LeakyReLU(0.2)(x)
    x = UpSampling1D(2)(x)  # 250

    x = Conv1D(64, 3, padding='same')(x)
    x = BatchNormalization()(x)
    x = LeakyReLU(0.2)(x)
    x = UpSampling1D(2)(x)  # 500

    x = Conv1D(32, 3, padding='same')(x)
    x = BatchNormalization()(x)
    x = LeakyReLU(0.2)(x)
    x = UpSampling1D(2)(x)  # 1000

    # Final reconstruction
    x = Conv1D(1, 3, padding='same', activation='linear')(x)

    model = Model(input_layer, x)

    return model

# 評估模型函數
def evaluate_model(model, X_test, y_test, input_data_interp, output_data_interp):
    # 獲取預測結果
    y_pred = model.predict(X_test)

    # 計算評估指標
    mse = mean_squared_error(y_test.flatten(), y_pred.flatten())
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test.flatten(), y_pred.flatten())

    # 計算皮爾遜相關係數
    correlation, _ = pearsonr(y_test.flatten(), y_pred.flatten())

    # 打印評估結果
    print("\nModel Evaluation Metrics:")
    print(f"Mean Squared Error (MSE): {mse:.4f}")
    print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
    print(f"R-squared (R2) Score: {r2:.4f}")
    print(f"Pearson Correlation Coefficient: {correlation:.4f}")

    return mse, rmse, r2, correlation

# 繪製結果函數
def plot_results(y_test, y_pred, history, num_samples=3):
    # 創建一個包含多個子圖的圖表
    plt.figure(figsize=(15, 10))

    # 繪製訓練歷史
    plt.subplot(2, 1, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Training History')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    # 繪製預測結果比較
    plt.subplot(2, 1, 2)
    for i in range(num_samples):
        plt.plot(y_test[i], label=f'True {i+1}', linestyle='-')
        plt.plot(y_pred[i], label=f'Predicted {i+1}', linestyle='--')

    plt.title('Comparison of True vs Predicted Values')
    plt.xlabel('Time Steps')
    plt.ylabel('Normalized Value')
    plt.legend()

    plt.tight_layout()
    plt.savefig('model_results.png')
    plt.close()

# 在原有的plot_results函數後添加以下新的繪圖函數

def plot_comprehensive_analysis(X_test, y_test, y_pred, input_data_interp, output_data_interp, history, num_samples=3):
    """
    提供更全面的視覺化分析，包括：
    1. 標準化數據的比較
    2. 還原後數據的比較
    3. 殘差分析
    4. 分布對比
    """
    plt.figure(figsize=(20, 15))

    # 1. 標準化數據比較
    plt.subplot(3, 2, 1)
    for i in range(num_samples):
        plt.plot(y_test[i], label=f'True {i+1}', linestyle='-')
        plt.plot(y_pred[i], label=f'Predicted {i+1}', linestyle='--')
    plt.title('Normalized Data Comparison')
    plt.xlabel('Time Steps')
    plt.ylabel('Normalized Value')
    plt.legend()

    # 2. 還原數據比較
    plt.subplot(3, 2, 2)
    for i in range(num_samples):
        # 還原數據
        true_mean = output_data_interp[i].mean()
        true_std = output_data_interp[i].std()

        true_original = y_test[i] * true_std + true_mean
        pred_original = y_pred[i] * true_std + true_mean

        plt.plot(true_original, label=f'True Original {i+1}', linestyle='-')
        plt.plot(pred_original, label=f'Predicted Original {i+1}', linestyle='--')
    plt.title('Restored Data Comparison')
    plt.xlabel('Time Steps')
    plt.ylabel('Original Value')
    plt.legend()

    # 5. 訓練歷史
    plt.subplot(3, 2, 3)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Training History')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    # 6. 散點圖比較
    plt.subplot(3, 2, 4)
    plt.scatter(y_test.flatten(), y_pred.flatten(), alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
    plt.title('Predicted vs True Values')
    plt.xlabel('True Values')
    plt.ylabel('Predicted Values')

    plt.tight_layout()
    plt.savefig('comprehensive_analysis.png')
    plt.close()

# 在main函數中添加以下代碼（在評估模型之後）:
def analyze_prediction_quality(y_test, y_pred, output_data_interp):
    """分析預測質量並輸出詳細統計信息"""
    # 計算每個樣本的統計指標
    sample_metrics = []
    for i in range(len(y_test)):
        true_mean = output_data_interp[i].mean()
        true_std = output_data_interp[i].std()

        # 還原原始值
        true_original = y_test[i] * true_std + true_mean
        pred_original = y_pred[i] * true_std + true_mean

        # 計算各種指標
        mse = mean_squared_error(true_original, pred_original)
        rmse = np.sqrt(mse)
        r2 = r2_score(true_original, pred_original)
        corr, _ = pearsonr(true_original.flatten(), pred_original.flatten())

        sample_metrics.append({
            'sample_id': i,
            'mse': mse,
            'rmse': rmse,
            'r2': r2,
            'correlation': corr,
            'mean_diff': abs(np.mean(true_original) - np.mean(pred_original)),
            'std_diff': abs(np.std(true_original) - np.std(pred_original))
        })

    # 轉換為DataFrame並輸出
    metrics_df = pd.DataFrame(sample_metrics)
    print("\nDetailed Prediction Quality Analysis:")
    print(metrics_df.describe())

    # 保存到CSV
    metrics_df.to_csv('prediction_quality_analysis.csv')
    return metrics_df

def plot_individual_results(X_test, y_test, y_pred, output_data_interp, save_dir='individual_results'):
    """為每個測試樣本生成並保存單獨的圖表"""
    os.makedirs(save_dir, exist_ok=True)

    for i in range(len(y_test)):
        plt.figure(figsize=(10, 6))

        # 還原數據
        true_mean = output_data_interp[i].mean()
        true_std = output_data_interp[i].std()
        true_original = y_test[i] * true_std + true_mean
        pred_original = y_pred[i] * true_std + true_mean

        # 繪製原始和預測數據
        plt.plot(true_original, label='True', linestyle='-')
        plt.plot(pred_original, label='Predicted', linestyle='--')
        plt.title(f'Sample {i + 1}: True vs Predicted')
        plt.xlabel('Time Steps')
        plt.ylabel('Original Value')
        plt.legend()

        # 保存圖表
        plt.savefig(os.path.join(save_dir, f'sample_{i + 1}.png'))
        plt.close()

# 主程序
def main():
    # 讀取數據
    input_file = '3dData_half_filtered.xlsx'
    output_file = 'elecData_half_filtered.xlsx'

    input_data = pd.read_excel(input_file, engine='openpyxl').to_numpy().T
    output_data = pd.read_excel(output_file, engine='openpyxl').to_numpy().T

    # 數據預處理
    input_length = 1000
    output_length = 1000

    # input_data_filtered = preprocess_data(input_data)
    # output_data_filtered = preprocess_data(output_data)

    input_data_interp = interpolate_ignore_nans(input_data, input_length)
    output_data_interp = interpolate_ignore_nans(output_data, output_length)

    # 標準化
    input_data_norm = (input_data_interp - input_data_interp.mean(axis=1, keepdims=True)) / (input_data_interp.std(axis=1, keepdims=True) + 1e-8)
    output_data_norm = (output_data_interp - output_data_interp.mean(axis=1, keepdims=True)) / (output_data_interp.std(axis=1, keepdims=True) + 1e-8)

    # 分割數據集
    X_temp, X_test, y_temp, y_test = train_test_split(input_data_norm, output_data_norm, test_size=0.2, random_state=42)
    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)

    # 數據增強
    X_train_aug, y_train_aug = augment_data(X_train, y_train, num_augmentations=1)

    # 重塑數據為3D格式
    X_train_aug = X_train_aug.reshape(-1, input_length, 1)
    X_val = X_val.reshape(-1, input_length, 1)
    X_test = X_test.reshape(-1, input_length, 1)

    # 創建和編譯模型
    model = create_improved_autoencoder(input_length)
    model.compile(
        optimizer=Adam(learning_rate=0.0005),
        loss=custom_peak_loss
    )

    # 回調函數
    callbacks = [
        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=8, min_lr=1e-6),
        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
    ]

    # 訓練模型
    history = model.fit(
        X_train_aug, y_train_aug,
        epochs=100,
        batch_size=16,
        validation_data=(X_val, y_val),
        callbacks=callbacks,
        verbose=1
    )

    # 評估模型
    mse, rmse, r2, correlation = evaluate_model(model, X_test, y_test, input_data_interp, output_data_interp)

    # 獲取預測結果並繪製
    y_pred = model.predict(X_test)
    analyze_prediction_quality(y_test, y_pred, output_data_interp)
    plot_results(y_test, y_pred, history)
    plot_comprehensive_analysis(X_test, y_test, y_pred, input_data_interp, output_data_interp, history, num_samples=3)
    plot_individual_results(X_test, y_test, y_pred, output_data_interp)

    # 保存模型
    model.save('improved_autoencoder.h5')

    return model, history, (mse, rmse, r2, correlation)

if __name__ == "__main__":
    model, history, metrics = main()
